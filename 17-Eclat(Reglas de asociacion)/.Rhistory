install.packages("caTools")
library(caTools)
#divide dataset into training and testing
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# Plantilla para el pre procesado de datos
#import dataset
dataset = read.csv("Data.csv")
#cleaning NAs
dataset$Age=ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary=ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
#transforming categorical data
dataset$Country=factor(dataset$Country,
levels=c("France","Spain","Germany"),
labels=c(1,2,3))
dataset$Purchased=factor(dataset$Purchased,
levels=c("No","Yes"),
labels=c(0,1))
#divide dataset into training and testing
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# Plantilla para el pre procesado de datos
#import dataset
dataset = read.csv("Data.csv")
#cleaning NAs
dataset$Age=ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary=ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
#transforming categorical data
dataset$Country=factor(dataset$Country,
levels=c("France","Spain","Germany"),
labels=c(1,2,3))
dataset$Purchased=factor(dataset$Purchased,
levels=c("No","Yes"),
labels=c(0,1))
#divide dataset into training and testing
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# Plantilla para el pre procesado de datos
#import dataset
dataset = read.csv("Data.csv")
#cleaning NAs
dataset$Age=ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary=ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
#transforming categorical data
dataset$Country=factor(dataset$Country,
levels=c("France","Spain","Germany"),
labels=c(1,2,3))
dataset$Purchased=factor(dataset$Purchased,
levels=c("No","Yes"),
labels=c(0,1))
#divide dataset into training and testing
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split==TRUE)
testing_set = subset(dataset, split==FALSE)
setwd("A:/Asus/Documents/UDEMYCOURSES/Machine Learning A to Z/Practicas/14-K-Means(Clustering)")
#Kmeans clustering
#import dataset
dataset = read.csv("Social_Network_Ads.csv")
#Kmeans clustering
#import dataset
dataset = read.csv("Mall_Customers.csv")
View(dataset)
X = dataset[,4:5]
View(X)
?kmeans
dataset = read.csv("Mall_Customers.csv")
X = dataset[,4:5]
#Metodo del codo
set.seed(6)
wcss = vector()
for(i in 1:10){
wcss[i] <- sum(kmeans(X, i)$withinss)
}
plot(1:10, wcss, type="b", main="Metodo del codo", xlab="Numero de clusters (k)",
ylab="WCSS(k)")
dataset = read.csv("Mall_Customers.csv")
X = dataset[,4:5]
#Metodo del codo
set.seed(6, sample.kind="Rounding")
wcss = vector()
for(i in 1:10){
wcss[i] <- sum(kmeans(X, i)$withinss)
}
plot(1:10, wcss, type="b", main="Metodo del codo", xlab="Numero de clusters (k)",
ylab="WCSS(k)")
dataset = read.csv("Mall_Customers.csv")
X = dataset[,4:5]
#Metodo del codo
set.seed(6)
wcss = vector()
for(i in 1:10){
wcss[i] <- kmeans(X, i)$tot.withinss
}
plot(1:10, wcss, type="b", main="Metodo del codo", xlab="Numero de clusters (k)",
ylab="WCSS(k)")
dataset = read.csv("Mall_Customers.csv")
X = dataset[,4:5]
#Metodo del codo
set.seed(6)
wcss = vector()
for(i in 1:10){
wcss[i] <- kmeans(X, i)$tot.withinss
}
plot(1:10, wcss, type="b", main="Metodo del codo", xlab="Numero de clusters (k)",
ylab="WCSS(k)")
set.seed(29)
kmeans <- kmeans(X, 5, iter.max = 300, nstart = 10)
?clusplot
??clusplot
?cluster
??cluster
set.seed(29)
kmeans <- kmeans(X, 5, iter.max = 300, nstart = 10)
#visualizacion de los clusters
library(cluster)
clusplot(X,
kmeans$cluster,
lines = 0,
shade = TRUE,
color = TRUE,
plotchar = FALSE,
span = TRUE,
main = "Cluster de clientes",
xlab = "Ingresos anuales en miles",
ylab = "Puntuacion (1-100)")
set.seed(29)
kmeans <- kmeans(X, 5, iter.max = 300, nstart = 10)
#visualizacion de los clusters
library(cluster)
clusplot(X,
kmeans$cluster,
lines = 0,
shade = TRUE,
color = TRUE,
plotchar = FALSE,
span = TRUE,
labels = 4,
main = "Cluster de clientes",
xlab = "Ingresos anuales en miles",
ylab = "Puntuacion (1-100)")
setwd("A:/Asus/Documents/UDEMYCOURSES/Machine Learning A to Z/Practicas/15-Clustering jerarquico(Clustering)")
dataset = read.csv("Mall_Customers.csv")
#Characteristic matrix
X = dataset[,4:5]
dataset = read.csv("Mall_Customers.csv")
#Characteristic matrix
X = dataset[,4:5]
?hclust
dendrogram = hclust(dist(X, method="euclidean"),
method = "ward.D")
plot(dendrogran,
main = "Dendrogram",
xlab = "Clientes del centro comercial",
ylab = "Distancia Euclidea")
#Utilizar el dendrograma para determinar el numero optimo de clusters
dendrogram = hclust(dist(X, method="euclidean"),
method = "ward.D")
plot(dendrogram,
main = "Dendrogram",
xlab = "Clientes del centro comercial",
ylab = "Distancia Euclidea")
y_hc = cutree(dendogram, k=5)
y_hc = cutree(dendrogram, k=5)
?hclust
library(cluster)
clusplot(X,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
plotchar = FALSE,
span = TRUE,
labels = 4,
main = "Cluster de clientes",
xlab = "Ingresos anuales en miles",
ylab = "Puntuacion (1-100)")
setwd("A:/Asus/Documents/UDEMYCOURSES/Machine Learning A to Z/Practicas/16-Apriori(Reglas de asociacion)")
dataset = read.csv("Market_Basket_Optimisation.csv")
dataset = read.csv("Market_Basket_Optimisation.csv")
View(dataset)
dataset = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
View(dataset)
View(dataset)
install.packages("arules")
setwd("A:/Asus/Documents/UDEMYCOURSES/Machine Learning A to Z/Practicas/16-Apriori(Reglas de asociacion)")
library(arules)
dataset = read.transactions("Market_Basket_Optimisation.csv",
sep = ",",
rm.duplicates = TRUE)
View(dataset)
summary(dataset)
itemFrequencyPlot(dataset, topN = 50)
itemFrequencyPlot(dataset, topN = 10)
#Entrenar algoritmo apriori con el dataset
rules = apriori(dataset, parameter = list(support = 0.003, confidence = 0.8))
#Entrenar algoritmo apriori con el dataset
rules = apriori(dataset, parameter = list(support = 0.003, confidence = 0.4))
View(rules)
summary(rules)
#Visualizacion de los resultados y las reglas
inspect(sort(rules, by = 'lift')[1:10])
#Visualizacion de los resultados y las reglas
inspect(sort(rules, by = 'lift')[1:10])
#Entrenar algoritmo apriori con el dataset
rules = apriori(dataset, parameter = list(support = 0.003, confidence = 0.2))
#Visualizacion de los resultados y las reglas
inspect(sort(rules, by = 'lift')[1:10])
#Entrenar algoritmo apriori con el dataset
rules = apriori(dataset, parameter = list(support = 0.004, confidence = 0.2))
#Visualizacion de los resultados y las reglas
inspect(sort(rules, by = 'lift')[1:10])
install.packages("arulesViz")
# libraries --------------------------------------------------------------
library(arules)
library(arulesViz)
# data -------------------------------------------------------------------
trans <- read.transactions(
"Market_Basket_Optimisation.csv",
sep = ",",
rm.duplicates = TRUE
)
# apriori algoirthm ------------------------------------------------------
rules <- apriori(
data = trans,
parameter = list(support = 0.004, confidence = 0.2)
)
# visualizations ---------------------------------------------------------
plot(rules, method = "graph", engine = "htmlwidget")
setwd("A:/Asus/Documents/UDEMYCOURSES/Machine Learning A to Z/Practicas/17-Eclat(Reglas de asociacion)")
#import dataset
dataset = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
#Crear una matriz dispersa
library(arules)
dataset = read.transactions("Market_Basket_Optimisation.csv",
sep = ",",
rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10) #Obtener un diagrama de barras de los 10 productos mas vendidos
rules = eclat(dataset, parameter = list(support = 0.004, minlen = 2))
#Visualizacion de los resultados y las reglas
inspect(sort(rules, by = 'lift')[1:10])
View(rules)
View(rules)
#Visualizacion de los resultados y las reglas
inspect(sort(rules, by = 'support')[1:10])
